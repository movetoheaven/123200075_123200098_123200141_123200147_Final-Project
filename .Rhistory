corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
return(corpus.tmp)
}
# How many dialogues?
length(ep4$dialogue)
length(ep5$dialogue)
length(ep6$dialogue)
# How many characters?
length(levels(ep4$character))
# How many characters?
length(levels(ep4$character))
length(levels(ep5$character))
length(levels(ep6$character))
str(ep4)
str(ep5)
str(ep6)
# How many characters?
length(levels(ep4$character))
length(levels(ep5$character))
length(levels(ep6$character))
library(tidyverse) # data manipulation
library(tm) # text mining
library(wordcloud) # word cloud generator
library(wordcloud2) # word cloud generator
library(tidytext) # text mining for word processing and sentiment analysis
library(reshape2) # reshapes a data frame
library(radarchart) # drawing the radar chart from a data frame
library(RWeka) # data mining tasks
library(knitr) # dynamic report generation
# Read the data
ep4 <- read.table("D:/R/finalproject/SW_EpisodeIV.txt")
ep5 <- read.table("D:/R/finalproject/SW_EpisodeV.txt")
ep6 <- read.table("D:/R/finalproject/SW_EpisodeVI.txt")
# Read the Lexicons (for sentiment classification)
bing <- read_csv("D:/R/finalproject/Bing.csv")
nrc <- read_csv("D:/R/finalproject/NRC.csv")
afinn <- read_csv("D:/R/finalproject/Afinn.csv")
str(ep4)
str(ep5)
str(ep6)
# Text transformations
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
v_stopwords <- c(stopwords("english"), c("thats","weve","hes","theres","ive","im",
"will","can","cant","dont","youve","us",
"youre","youll","theyre","whats","didnt"))
corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
return(corpus.tmp)
}
# Most frequent terms
frequentTerms <- function(text){
s.cor <- Corpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Define bigram tokenizer
tokenizer  <- function(x){
NGramTokenizer(x, Weka_control(min=2, max=2))
}
# Most frequent bigrams
frequentBigrams <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer))
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# How many dialogues?
length(ep4$dialogue)
length(ep5$dialogue)
length(ep6$dialogue)
# How many characters?
length(levels(ep4$character))
length(levels(ep5$character))
length(levels(ep6$character))
# Top 20 characters with more dialogues
top.ep4.chars <- as.data.frame(sort(table(ep4$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.ep4.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for Episode IV
wordcloud2(frequentTerms(ep4$dialogue), size=0.5,
figPath="D:/R/finalproject/vader.png")
# Most frequent bigrams
ep4.bigrams <- frequentBigrams(ep4$dialogue)[1:20,]
ggplot(data=ep4.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# How many dialogues?
length(ep5$dialogue)
# How many characters?
length(levels(ep5$character))
# Top 20 characters with more dialogues
top.ep5.chars <- as.data.frame(sort(table(ep5$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.ep5.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for Episode V
wordcloud2(frequentTerms(ep5$dialogue), size=0.5,
figPath="D:/R/finalproject/yoda.png")
# Most frequent bigrams
ep5.bigrams <- frequentBigrams(ep5$dialogue)[1:20,]
ggplot(data=ep5.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# How many dialogues?
length(ep6$dialogue)
# How many characters?
length(levels(ep6$character))
# Top 20 characters with more dialogues
top.ep6.chars <- as.data.frame(sort(table(ep6$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.ep6.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for Episode VI
wordcloud2(frequentTerms(ep6$dialogue), size=0.5,
figPath="D:/R/finalproject/r2d2.png")
# Most frequent bigrams
ep6.bigrams <- frequentBigrams(ep6$dialogue)[1:20,]
ggplot(data=ep6.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# The Original Trilogy dialogues
trilogy <- rbind(ep4, ep5, ep6)
# How many dialogues?
length(trilogy$dialogue)
# How many characters?
length(levels(trilogy$character))
# Top 20 characters with more dialogues
top.trilogy.chars <- as.data.frame(sort(table(trilogy$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.trilogy.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for The Original Trilogy
wordcloud2(frequentTerms(trilogy$dialogue), size=0.4)
figPath=("D:/R/finalproject/rebel alliance.png")
# Most frequent bigrams
trilogy.bigrams <- frequentBigrams(trilogy$dialogue)[1:20,]
ggplot(data=trilogy.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
unnest_tokens(word, dialogue)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Sentiments and frequency associated with each word
sentiments <- tokens %>%
inner_join(nrc, "word") %>%
count(word, sentiment, sort=TRUE)
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw()
# How many characters?
length(levels(trilogy$characters))
# Top 10 terms for each sentiment
sentiments %>%
group_by(sentiment) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(x=reorder(word, n), y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y="Frequency", x="Terms") +
coord_flip() +
theme_bw()
# Sentiment analysis for the Top 10 characters with more dialogues
tokens %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
inner_join(nrc, "word") %>%
count(character, sentiment, sort=TRUE) %>%
ggplot(aes(x=sentiment, y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~character, scales="free_x") +
labs(x="Sentiment", y="Frequency") +
coord_flip() +
theme_bw()
# Stopwords
mystopwords <- data_frame(word=c(stopwords("english"),
c("thats","weve","hes","theres","ive","im",
"will","can","cant","dont","youve","us",
"youre","youll","theyre","whats","didnt")))
# Tokens without stopwords
top.chars.tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
unnest_tokens(word, dialogue) %>%
anti_join(mystopwords, by="word")
# Most frequent words for each character
top.chars.tokens %>%
count(character, word) %>%
group_by(character) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=n)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
labs(x="Sentiment", y="Frequency") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Most relevant words for each character
top.chars.tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(y="tfâ€“idf", x="Sentiment") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
library(tidyverse) # data manipulation
library(tm) # text mining
library(wordcloud) # word cloud generator
library(wordcloud2) # word cloud generator
library(tidytext) # text mining for word processing and sentiment analysis
library(reshape2) # reshapes a data frame
library(radarchart) # drawing the radar chart from a data frame
library(RWeka) # data mining tasks
library(knitr) # dynamic report generation
# Read the data
ep4 <- read.table("D:/R/finalproject/SW_EpisodeIV.txt")
ep5 <- read.table("D:/R/finalproject/SW_EpisodeV.txt")
ep6 <- read.table("D:/R/finalproject/SW_EpisodeVI.txt")
# Read the Lexicons (for sentiment classification)
bing <- read_csv("D:/R/finalproject/Bing.csv")
nrc <- read_csv("D:/R/finalproject/NRC.csv")
afinn <- read_csv("D:/R/finalproject/Afinn.csv")
str(ep4)
str(ep5)
str(ep6)
# Text transformations
cleanCorpus <- function(corpus){
corpus.tmp <- tm_map(corpus, removePunctuation)
corpus.tmp <- tm_map(corpus.tmp, stripWhitespace)
corpus.tmp <- tm_map(corpus.tmp, content_transformer(tolower))
v_stopwords <- c(stopwords("english"), c("thats","weve","hes","theres","ive","im",
"will","can","cant","dont","youve","us",
"youre","youll","theyre","whats","didnt"))
corpus.tmp <- tm_map(corpus.tmp, removeWords, v_stopwords)
corpus.tmp <- tm_map(corpus.tmp, removeNumbers)
return(corpus.tmp)
}
# Most frequent terms
frequentTerms <- function(text){
s.cor <- Corpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl)
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# Define bigram tokenizer
tokenizer  <- function(x){
NGramTokenizer(x, Weka_control(min=2, max=2))
}
# Most frequent bigrams
frequentBigrams <- function(text){
s.cor <- VCorpus(VectorSource(text))
s.cor.cl <- cleanCorpus(s.cor)
s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer))
s.tdm <- removeSparseTerms(s.tdm, 0.999)
m <- as.matrix(s.tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word=names(word_freqs), freq=word_freqs)
return(dm)
}
# How many dialogues?
length(ep4$dialogue)
length(ep5$dialogue)
length(ep6$dialogue)
# How many characters?
length(levels(ep4$character))
length(levels(ep5$character))
length(levels(ep6$character))
# Top 20 characters with more dialogues
top.ep4.chars <- as.data.frame(sort(table(ep4$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.ep4.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for Episode IV
wordcloud2(frequentTerms(ep4$dialogue), size=0.5,
figPath="D:/R/finalproject/vader.png")
# Most frequent bigrams
ep4.bigrams <- frequentBigrams(ep4$dialogue)[1:20,]
ggplot(data=ep4.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# How many dialogues?
length(ep5$dialogue)
# How many characters?
length(levels(ep5$character))
# Top 20 characters with more dialogues
top.ep5.chars <- as.data.frame(sort(table(ep5$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.ep5.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for Episode V
wordcloud2(frequentTerms(ep5$dialogue), size=0.5,
figPath="D:/R/finalproject/yoda.png")
# Most frequent bigrams
ep5.bigrams <- frequentBigrams(ep5$dialogue)[1:20,]
ggplot(data=ep5.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# How many dialogues?
length(ep6$dialogue)
# How many characters?
length(levels(ep6$character))
# Top 20 characters with more dialogues
top.ep6.chars <- as.data.frame(sort(table(ep6$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.ep6.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for Episode VI
wordcloud2(frequentTerms(ep6$dialogue), size=0.5,
figPath="D:/R/finalproject/r2d2.png")
# Most frequent bigrams
ep6.bigrams <- frequentBigrams(ep6$dialogue)[1:20,]
ggplot(data=ep6.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# The Original Trilogy dialogues
trilogy <- rbind(ep4, ep5, ep6)
# How many dialogues?
length(trilogy$dialogue)
# How many characters?
length(levels(trilogy$character))
# Top 20 characters with more dialogues
top.trilogy.chars <- as.data.frame(sort(table(trilogy$character), decreasing=TRUE))[1:20,]
# Visualization
ggplot(data=top.trilogy.chars, aes(x=Var1, y=Freq)) +
geom_bar(stat="identity", fill="#56B4E9", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Character", y="Number of dialogues")
# Wordcloud for The Original Trilogy
wordcloud2(frequentTerms(trilogy$dialogue), size=0.4)
figPath=("D:/R/finalproject/rebel alliance.png")
# Most frequent bigrams
trilogy.bigrams <- frequentBigrams(trilogy$dialogue)[1:20,]
ggplot(data=trilogy.bigrams, aes(x=reorder(word, -freq), y=freq)) +
geom_bar(stat="identity", fill="chocolate2", colour="black") +
theme_bw() +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(x="Bigram", y="Frequency")
# Transform the text to a tidy data structure with one token per row
tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
unnest_tokens(word, dialogue)
# Positive and negative words
tokens %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort=TRUE) %>%
acast(word ~ sentiment, value.var="n", fill=0) %>%
comparison.cloud(colors=c("#F8766D", "#00BFC4"), max.words=100)
# Sentiments and frequency associated with each word
sentiments <- tokens %>%
inner_join(nrc, "word") %>%
count(word, sentiment, sort=TRUE)
# Frequency of each sentiment
ggplot(data=sentiments, aes(x=reorder(sentiment, -n, sum), y=n)) +
geom_bar(stat="identity", aes(fill=sentiment), show.legend=FALSE) +
labs(x="Sentiment", y="Frequency") +
theme_bw()
# Top 10 terms for each sentiment
sentiments %>%
group_by(sentiment) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ggplot(aes(x=reorder(word, n), y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~sentiment, scales="free_y") +
labs(y="Frequency", x="Terms") +
coord_flip() +
theme_bw()
# Sentiment analysis for the Top 10 characters with more dialogues
tokens %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
inner_join(nrc, "word") %>%
count(character, sentiment, sort=TRUE) %>%
ggplot(aes(x=sentiment, y=n)) +
geom_col(aes(fill=sentiment), show.legend=FALSE) +
facet_wrap(~character, scales="free_x") +
labs(x="Sentiment", y="Frequency") +
coord_flip() +
theme_bw()
# Stopwords
mystopwords <- data_frame(word=c(stopwords("english"),
c("thats","weve","hes","theres","ive","im",
"will","can","cant","dont","youve","us",
"youre","youll","theyre","whats","didnt")))
# Tokens without stopwords
top.chars.tokens <- trilogy %>%
mutate(dialogue=as.character(trilogy$dialogue)) %>%
filter(character %in% c("LUKE","HAN","THREEPIO","LEIA","VADER",
"BEN","LANDO","YODA","EMPEROR","RED LEADER")) %>%
unnest_tokens(word, dialogue) %>%
anti_join(mystopwords, by="word")
# Most frequent words for each character
top.chars.tokens %>%
count(character, word) %>%
group_by(character) %>%
arrange(desc(n)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=n)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
labs(x="Sentiment", y="Frequency") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# Most relevant words for each character
top.chars.tokens %>%
count(character, word) %>%
bind_tf_idf(word, character, n) %>%
group_by(character) %>%
arrange(desc(tf_idf)) %>%
slice(1:10) %>%
ungroup() %>%
mutate(word2=factor(paste(word, character, sep="__"),
levels=rev(paste(word, character, sep="__"))))%>%
ggplot(aes(x=word2, y=tf_idf)) +
geom_col(aes(fill=character), show.legend=FALSE) +
facet_wrap(~character, scales="free_y") +
theme(axis.text.x=element_text(angle=45, hjust=1)) +
labs(y="tfâ€“idf", x="Sentiment") +
scale_x_discrete(labels=function(x) gsub("__.+$", "", x)) +
coord_flip() +
theme_bw()
# How many dialogues?
length(ep4$dialogue)
# How many characters?
length(chr(ep4$character))
View(ep4)
# How many characters?
length(`levels<-.factor`(ep4$character))
# How many characters?
length(levels(ep4$character))
